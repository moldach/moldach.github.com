<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Matthew J. Oldach on Matthew J. Oldach</title>
    <link>/</link>
    <description>Recent content in Matthew J. Oldach on Matthew J. Oldach</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>TensorFlow CNN for Fast Style Transfer</title>
      <link>/post/tensorflow-cnn-for-fast-style-transfer/</link>
      <pubDate>Sun, 21 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tensorflow-cnn-for-fast-style-transfer/</guid>
      <description>


&lt;p&gt;Volcom was founded in 1991, it was the first company to combine skateboarding, surfing and snowboarding under one brand from it’s incepetion. I always loved how their aesthethics captured the energy and artistry of board-riding in its purest form.&lt;/p&gt;
&lt;p&gt;If you have no idea what I’m talking about here’s some samples from print advertisments and a clip from 2003’s &lt;a href=&#34;https://www.youtube.com/watch?v=VGBQw46BbWA&#34;&gt;&lt;strong&gt;Big Youth Happening&lt;/strong&gt;&lt;/a&gt; which I would say was the ultimate inspiration for this post.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;volcom.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;bigYouth.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Non-photorealistic rendering (&lt;a href=&#34;https://books.google.ca/books?id=YjXlTPFYGEYC&amp;amp;redir_esc=y&#34;&gt;NPR&lt;/a&gt;) is a combination of computer graphics and computer vision that produces renderings in various artistic, expressive or stylized ways. A new art form that couldn’t have existed without computers. Researchers have proposed many algorithms and &lt;em&gt;styles&lt;/em&gt; such as painting, pen-and-ink drawing, tile mosaics, stippling, streamline visualization, and tensor field visualization. Although the details vary, the goal is to make an image look like some other image. Two main approaches to designing these algorithms exist. &lt;em&gt;Greedy algorithms&lt;/em&gt; greedily place strokes to match the target goas. &lt;em&gt;Optimization algorithms&lt;/em&gt; iteratively place and then adjust stroke positions to minimize the objective function. NPR can be done using a number of machine learning methods, most commonly &lt;a href=&#34;https://vimeo.com/302584040&#34;&gt;General Adverserial Networks&lt;/a&gt; or &lt;a href=&#34;https://www.youtube.com/watch?v=UFffxcCQMPQ&#34;&gt;Convolution Neural Networks&lt;/a&gt;(CNN).
such as painting and drawing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;example.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I wanted to use &lt;a href=&#34;https://github.com/lengstrom/fast-style-transfer&#34;&gt;lengstrom/fast-style-transfer&lt;/a&gt;, an &lt;em&gt;optimization technique&lt;/em&gt;, on a video of my friends and I backcountry skiing and snowboarding in The Rocky Mountains of Alberta.&lt;/p&gt;
&lt;p&gt;Fast-style-transfer is a TensorFlow CNN model based on a combination of &lt;a href=&#34;https://arxiv.org/abs/1508.06576&#34;&gt;Gatys’ A Neural Algorithm of Artistic Style&lt;/a&gt;, Johnson’s &lt;a href=&#34;http://cs.stanford.edu/people/jcjohns/eccv16/&#34;&gt;Perceptual Losses for Real-Time Style Transfer and Super-Resolution&lt;/a&gt;, and Ulyanov’s &lt;a href=&#34;https://arxiv.org/abs/1607.08022&#34;&gt;Instance Normalization&lt;/a&gt;. I’ll skip the details here because it’s been been far better described in the publications above (or in layman’s terms &lt;a href=&#34;https://towardsdatascience.com/a-journey-into-convolutional-neural-network-visualization-1abc71605209&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398&#34;&gt;here&lt;/a&gt;, and &lt;a href=&#34;https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1&#34;&gt;here&lt;/a&gt;), suffice it to say it allows you to compose images/videos in the style of another image. Besides being fun it’s a nice visual way of showing the capabilities and internal representations of neural networks.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;fast-style-transfer&lt;/code&gt; repo comes with 6 trained models on famous paintings, allowing you to quikcly style your images (or videos) like them. But what’s the fun in doing what others have alread done?&lt;/p&gt;
&lt;p&gt;I wanted to see what kind of effect would be produced by training style transfer networks around interesting shapes and patterns! I selected a number patterns I thought would be interesting, for example animal patterns, fractals, fibonnaci sequences, a few vaporwave images (&lt;a href=&#34;https://github.com/moldach/vapoRwave&#34;&gt;of course&lt;/a&gt;), and a bunch of images taken under a &lt;a href=&#34;https://en.wikipedia.org/wiki/Scanning_electron_microscope&#34;&gt;SEM microscope&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;collage.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I then constructed a script to train 50 models simultaneously on a HPC (using a &lt;a href=&#34;https://images.nvidia.com/content/technologies/volta/pdf/tesla-volta-v100-datasheet-letter-fnl-web.pdf&#34;&gt;GPU NVIDIA V100 Volta&lt;/a&gt;; a top-of-the-line $10,000 GPU - &lt;em&gt;spared no expense&lt;/em&gt;). After &lt;em&gt;many many&lt;/em&gt; hours of training, across hundreds of cores, I selected the 20 best (&lt;em&gt;i.e.&lt;/em&gt; most aesthetically pleasing) models to apply to the video.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;mountains.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;The entire high-quality video can be found here&lt;/strong&gt;](&lt;a href=&#34;https://youtu.be/TuN4456PK-c&#34; class=&#34;uri&#34;&gt;https://youtu.be/TuN4456PK-c&lt;/a&gt;).&lt;/p&gt;
&lt;div id=&#34;technical-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Technical Notes:&lt;/h3&gt;
&lt;p&gt;When training the models I noticed that the checkpoint files were being corrupted. I found a &lt;a href=&#34;https://github.com/lengstrom/fast-style-transfer/issues/78#issuecomment-302307215&#34;&gt;closed issue on the repo: #78&lt;/a&gt; stating that Tensorflow’s Saver class was updated so you’ll need to change line 136 in &lt;code&gt;/src/optimize.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The script for training all of the networks on an HPC with a SLURM job scheduler is below, and can be run with &lt;code&gt;bash fastTrainer.bash &amp;lt;image_path&amp;gt; &amp;lt;out_path&amp;gt; &amp;lt;test_path&amp;gt;&lt;/code&gt;. Simply clone &lt;a href=&#34;https://github.com/lengstrom/fast-style-transfer&#34;&gt;lengstrom/fast-style-transfer&lt;/a&gt; and create three folders inside. Create a sub-directory where you’ll put all the &lt;strong&gt;style images&lt;/strong&gt; you want to train, another sub-directory for the &lt;strong&gt;output&lt;/strong&gt; (&lt;code&gt;.ckpt&lt;/code&gt; files used to style your &lt;code&gt;.mp4&lt;/code&gt; videos), and a &lt;strong&gt;test&lt;/strong&gt; sub-directory for the model. As a default I used the &lt;code&gt;chicago&lt;/code&gt; for training and I’m not entirely sure if that makes a difference.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;!/bin/bash
#$ -pwd

# bash fastTrainer.bash /images/ /outpath/ /testpath/
##
## An embarrassingly parallel script to train many style transfer networks on a HPC
## Access to SLURM job scheduler and fast-style-transfer is required to run this program.
## The three mandatory pathways must be specified in the indicated order.

IMG=$(readlink -f &amp;quot;${1%/}&amp;quot;)     # path_to_train_images
OUT_DIR=$(readlink -f &amp;quot;${2%/}&amp;quot;)  # path_to_checkpoints
TEST=$(readlink -f &amp;quot;${3%/}&amp;quot;)  # path_to_tests

mkdir -p ${OUT_DIR}/jobs

JID=0   # job ID for SLURM job name

for f in ${IMG}/*; do

        let JID=(JID+1)

  cat &amp;gt; ${OUT_DIR}/jobs/style_${JID}.bash &amp;lt;&amp;lt; EOT # write job information for each job
#!/bin/bash
#SBATCH --gres=gpu:1        # request GPU
#SBATCH --cpus-per-task=2   # maximum CPU cores per GPU request
#SBATCH --time=00:01:00     # request 8 hours of walltime
#SBATCH --job-name=&amp;quot;fst_${JID}&amp;quot;
#SBATCH --output=${OUT_DIR}/jobs/%N-%j.out  # %N for node name, %j for jobID

### JOB SCRIPT BELLOW ###

# Load Modules
module load python/2.7.14
module load scipy-stack
source tensorflow/bin/activate

mkdir ${OUT_DIR}/${JID}
mkdir ${TEST}/${JID}

python style.py --style $f \
  --checkpoint-dir ${OUT_DIR}/${JID} \
  --test examples/content/chicago.jpg \
  --test-dir ${OUT_DIR}/${JID} \
  --content-weight 1.5e1 \
  --checkpoint-iterations 1000 \
  --batch-size 20

EOT
  chmod 754 $(readlink -f &amp;quot;${OUT_DIR}&amp;quot;)/jobs/style_${JID}.bash
  sbatch $(readlink -f &amp;quot;${OUT_DIR}&amp;quot;)/jobs/style_${JID}.bash
done
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-i-learned&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What I learned&lt;/h2&gt;
&lt;p&gt;Now &lt;a href=&#34;https://www.technologyreview.com/s/612913/a-philosopher-argues-that-an-ai-can-never-be-an-artist/&#34;&gt;Sean Dorrance Kelly&lt;/a&gt; might argue I’m using the machines “creativity” as a subsititute for my own but I would disagree since there was a fair amount of creativity in selecting images to train models on and the editing itself. That being said an &lt;a href=&#34;https://www.christies.com/features/A-collaboration-between-two-artists-one-human-one-a-machine-9332-1.aspx&#34;&gt;AI generated work of art recently sold for $432,500 at Christie’s&lt;/a&gt; (the first auction house to offer a piece of art created by an algorithm).&lt;/p&gt;
&lt;p&gt;Besides the opportunity to exercise my right-brain, it was a good chance to gain some experience with tensorflow and python. This project was also illuminating because it made the hidden layers of networks more comprehensible by literally allowing me to &lt;em&gt;see&lt;/em&gt; how neural networks are performing!&lt;/p&gt;
&lt;p&gt;I think the field of &lt;strong&gt;Feature visualization&lt;/strong&gt; is exciting because it allows to peer behind the curtain and see how networks learn to classify images accurately. Take for example the &lt;a href=&#34;https://distill.pub/2019/activation-atlas/?utm_campaign=Data_Elixir&amp;amp;utm_medium=email&amp;amp;utm_source=Data_Elixir_224&#34;&gt;Activation atlas&lt;/a&gt; which reveals visual abstractions within a model. It gives a global view of a dataset by showing feature visualization of averaged activation values from a neural network.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;atlas.png&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As an analogy, while the 26 letters in the alphabet provide a basis for English, seeing how letters are commonly combined to make words gives far more insight into the concepts that can be expressed than the letters alone. Similarly, activation atlases give us a bigger picture view by showing common combinations of neurons.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In a perfect world the artist/user should have control over the decisions made by the algorithm. For example, to specify spatially varying styles to use, so that different rendering styles are used in different parts of the image, or to specify positions of individuial strokes. However, training these models is still too slow to be useful in an interactive application.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>TensorFlow CNN for Fast Style Transfer</title>
      <link>/post/faststyletransfer/</link>
      <pubDate>Sun, 21 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/faststyletransfer/</guid>
      <description>

&lt;p&gt;Volcom was founded in 1991, it was the first company to combine skateboarding, surfing and snowboarding under one brand from it&amp;rsquo;s incepetion. I always loved how their aesthethics captured the energy and artistry of board-riding in its purest form.&lt;/p&gt;

&lt;p&gt;If you have no idea what I&amp;rsquo;m talking about here&amp;rsquo;s some samples from print advertisments and a clip from 2003&amp;rsquo;s &lt;a href=&#34;https://www.youtube.com/watch?v=VGBQw46BbWA&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Big Youth Happening&lt;/strong&gt;&lt;/a&gt; which I would say was the ultimate inspiration for this post.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;volcom.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;bigYouth.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Non-photorealistic rendering (&lt;a href=&#34;https://books.google.ca/books?id=YjXlTPFYGEYC&amp;amp;redir_esc=y&#34; target=&#34;_blank&#34;&gt;NPR&lt;/a&gt;) is a combination of computer graphics and computer vision that produces renderings in various artistic, expressive or stylized ways. A new art form that couldn&amp;rsquo;t have existed without computers. Researchers have proposed many algorithms and &lt;em&gt;styles&lt;/em&gt; such as painting, pen-and-ink drawing, tile mosaics, stippling, streamline visualization, and tensor field visualization. Although the details vary, the goal is to make an image look like some other image. Two main approaches to designing these algorithms exist. &lt;em&gt;Greedy algorithms&lt;/em&gt; greedily place strokes to match the target goas. &lt;em&gt;Optimization algorithms&lt;/em&gt; iteratively place and then adjust stroke positions to minimize the objective function. NPR can be done using a number of machine learning methods, most commonly &lt;a href=&#34;https://vimeo.com/302584040&#34; target=&#34;_blank&#34;&gt;General Adverserial Networks&lt;/a&gt; or &lt;a href=&#34;https://www.youtube.com/watch?v=UFffxcCQMPQ&#34; target=&#34;_blank&#34;&gt;Convolution Neural Networks&lt;/a&gt;(CNN).
such as painting and drawing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;example.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I wanted to use &lt;a href=&#34;https://github.com/lengstrom/fast-style-transfer&#34; target=&#34;_blank&#34;&gt;lengstrom/fast-style-transfer&lt;/a&gt;, an &lt;em&gt;optimization technique&lt;/em&gt;, on a video of my friends and I backcountry skiing and snowboarding in The Rocky Mountains of Alberta.&lt;/p&gt;

&lt;p&gt;Fast-style-transfer is a TensorFlow CNN model based on a combination of &lt;a href=&#34;https://arxiv.org/abs/1508.06576&#34; target=&#34;_blank&#34;&gt;Gatys&amp;rsquo; A Neural Algorithm of Artistic Style&lt;/a&gt;, Johnson&amp;rsquo;s &lt;a href=&#34;http://cs.stanford.edu/people/jcjohns/eccv16/&#34; target=&#34;_blank&#34;&gt;Perceptual Losses for Real-Time Style Transfer and Super-Resolution&lt;/a&gt;, and Ulyanov&amp;rsquo;s &lt;a href=&#34;https://arxiv.org/abs/1607.08022&#34; target=&#34;_blank&#34;&gt;Instance Normalization&lt;/a&gt;. I&amp;rsquo;ll skip the details here because it&amp;rsquo;s been been far better described in the publications above (or in layman&amp;rsquo;s terms &lt;a href=&#34;https://towardsdatascience.com/a-journey-into-convolutional-neural-network-visualization-1abc71605209&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;, and &lt;a href=&#34;https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;), suffice it to say it allows you to compose images/videos in the style of another image. Besides being fun it&amp;rsquo;s a nice visual way of showing the capabilities and internal representations of neural networks.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;fast-style-transfer&lt;/code&gt; repo comes with 6 trained models on famous paintings, allowing you to quikcly style your images (or videos) like them. But what&amp;rsquo;s the fun in doing what others have alread done?&lt;/p&gt;

&lt;p&gt;I wanted to see what kind of effect would be produced by training style transfer networks around interesting shapes and patterns! I selected a number patterns I thought would be interesting, for example animal patterns, fractals, fibonnaci sequences, a few vaporwave images (&lt;a href=&#34;https://github.com/moldach/vapoRwave&#34; target=&#34;_blank&#34;&gt;of course&lt;/a&gt;), and a bunch of images taken under a &lt;a href=&#34;https://en.wikipedia.org/wiki/Scanning_electron_microscope&#34; target=&#34;_blank&#34;&gt;SEM microscope&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;collage.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I then constructed a script to train 50 models simultaneously on a HPC (using a &lt;a href=&#34;https://images.nvidia.com/content/technologies/volta/pdf/tesla-volta-v100-datasheet-letter-fnl-web.pdf&#34; target=&#34;_blank&#34;&gt;GPU NVIDIA V100 Volta&lt;/a&gt;; a top-of-the-line $10,000 GPU - &lt;em&gt;spared no expense&lt;/em&gt;). After &lt;em&gt;many many&lt;/em&gt; hours of training, across hundreds of cores, I selected the 20 best (&lt;em&gt;i.e.&lt;/em&gt; most aesthetically pleasing) models to apply to the video.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;mountains.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;(&lt;strong&gt;The entire high-quality video can be found here&lt;/strong&gt;](&lt;a href=&#34;https://youtu.be/TuN4456PK-c&#34; target=&#34;_blank&#34;&gt;https://youtu.be/TuN4456PK-c&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&#34;technical-notes&#34;&gt;Technical Notes:&lt;/h3&gt;

&lt;p&gt;When training the models I noticed that the checkpoint files were being corrupted. I found a &lt;a href=&#34;https://github.com/lengstrom/fast-style-transfer/issues/78#issuecomment-302307215&#34; target=&#34;_blank&#34;&gt;closed issue on the repo: #78&lt;/a&gt; stating that Tensorflow&amp;rsquo;s Saver class was updated so you&amp;rsquo;ll need to change line 136 in &lt;code&gt;/src/optimize.py&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The script for training all of the networks on an HPC with a SLURM job scheduler is below, and can be run with &lt;code&gt;bash fastTrainer.bash &amp;lt;image_path&amp;gt; &amp;lt;out_path&amp;gt; &amp;lt;test_path&amp;gt;&lt;/code&gt;. Simply clone &lt;a href=&#34;https://github.com/lengstrom/fast-style-transfer&#34; target=&#34;_blank&#34;&gt;lengstrom/fast-style-transfer&lt;/a&gt; and create three folders inside. Create a sub-directory where you&amp;rsquo;ll put all the &lt;strong&gt;style images&lt;/strong&gt; you want to train, another sub-directory for the &lt;strong&gt;output&lt;/strong&gt; (&lt;code&gt;.ckpt&lt;/code&gt; files used to style your &lt;code&gt;.mp4&lt;/code&gt; videos), and a &lt;strong&gt;test&lt;/strong&gt; sub-directory for the model. As a default I used the &lt;code&gt;chicago&lt;/code&gt; for training and I&amp;rsquo;m not entirely sure if that makes a difference.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;!/bin/bash
#$ -pwd

# bash fastTrainer.bash /images/ /outpath/ /testpath/
##
## An embarrassingly parallel script to train many style transfer networks on a HPC
## Access to SLURM job scheduler and fast-style-transfer is required to run this program.
## The three mandatory pathways must be specified in the indicated order.

IMG=$(readlink -f &amp;quot;${1%/}&amp;quot;)     # path_to_train_images
OUT_DIR=$(readlink -f &amp;quot;${2%/}&amp;quot;)  # path_to_checkpoints
TEST=$(readlink -f &amp;quot;${3%/}&amp;quot;)  # path_to_tests

mkdir -p ${OUT_DIR}/jobs

JID=0   # job ID for SLURM job name

for f in ${IMG}/*; do

        let JID=(JID+1)

  cat &amp;gt; ${OUT_DIR}/jobs/style_${JID}.bash &amp;lt;&amp;lt; EOT # write job information for each job
#!/bin/bash
#SBATCH --gres=gpu:1        # request GPU
#SBATCH --cpus-per-task=2   # maximum CPU cores per GPU request
#SBATCH --time=00:01:00     # request 8 hours of walltime
#SBATCH --job-name=&amp;quot;fst_${JID}&amp;quot;
#SBATCH --output=${OUT_DIR}/jobs/%N-%j.out  # %N for node name, %j for jobID

### JOB SCRIPT BELLOW ###

# Load Modules
module load python/2.7.14
module load scipy-stack
source tensorflow/bin/activate

mkdir ${OUT_DIR}/${JID}
mkdir ${TEST}/${JID}

python style.py --style $f \
  --checkpoint-dir ${OUT_DIR}/${JID} \
  --test examples/content/chicago.jpg \
  --test-dir ${OUT_DIR}/${JID} \
  --content-weight 1.5e1 \
  --checkpoint-iterations 1000 \
  --batch-size 20

EOT
  chmod 754 $(readlink -f &amp;quot;${OUT_DIR}&amp;quot;)/jobs/style_${JID}.bash
  sbatch $(readlink -f &amp;quot;${OUT_DIR}&amp;quot;)/jobs/style_${JID}.bash
done

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;what-i-learned&#34;&gt;What I learned&lt;/h2&gt;

&lt;p&gt;Now &lt;a href=&#34;https://www.technologyreview.com/s/612913/a-philosopher-argues-that-an-ai-can-never-be-an-artist/&#34; target=&#34;_blank&#34;&gt;Sean Dorrance Kelly&lt;/a&gt; might argue I&amp;rsquo;m using the machines &amp;ldquo;creativity&amp;rdquo; as a subsititute for my own but I would disagree since there was a fair amount of creativity in selecting images to train models on and the editing itself. That being said an &lt;a href=&#34;https://www.christies.com/features/A-collaboration-between-two-artists-one-human-one-a-machine-9332-1.aspx&#34; target=&#34;_blank&#34;&gt;AI generated work of art recently sold for $432,500 at Christie&amp;rsquo;s&lt;/a&gt; (the first auction house to offer a piece of art created by an algorithm).&lt;/p&gt;

&lt;p&gt;Besides the opportunity to exercise my right-brain, it was a good chance to gain some experience with tensorflow and python. This project was also illuminating because it made the hidden layers of networks more comprehensible by literally allowing me to &lt;em&gt;see&lt;/em&gt; how neural networks are performing!&lt;/p&gt;

&lt;p&gt;I think the field of &lt;strong&gt;Feature visualization&lt;/strong&gt; is exciting because it allows to peer behind the curtain and see how networks learn to classify images accurately. Take for example the &lt;a href=&#34;https://distill.pub/2019/activation-atlas/?utm_campaign=Data_Elixir&amp;amp;utm_medium=email&amp;amp;utm_source=Data_Elixir_224&#34; target=&#34;_blank&#34;&gt;Activation atlas&lt;/a&gt; which reveals visual abstractions within a model. It gives a global view of a dataset by showing feature visualization of averaged activation values from a neural network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;atlas.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;As an analogy, while the 26 letters in the alphabet provide a basis for English, seeing how letters are commonly combined to make words gives far more insight into the concepts that can be expressed than the letters alone. Similarly, activation atlases give us a bigger picture view by showing common combinations of neurons.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In a perfect world the artist/user should have control over the decisions made by the algorithm. For example, to specify spatially varying styles to use, so that different rendering styles are used in different parts of the image, or to specify positions of individuial strokes. However, training these models is still too slow to be useful in an interactive application.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Step aside Cron, this is a job for .Rprofile &amp; logfiles!</title>
      <link>/post/rprofile/</link>
      <pubDate>Tue, 02 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/rprofile/</guid>
      <description>

&lt;p&gt;Just as &lt;a href=&#34;https://github.com/r-lib/testthat&#34; target=&#34;_blank&#34;&gt;test automation&lt;/a&gt; is a critical part of &lt;a href=&#34;http://users.atw.hu/sustainsoftdev/ch05lev1sec2.html&#34; target=&#34;_blank&#34;&gt;rutheless testing&lt;/a&gt;, your goal for any project should be to identify taks that can be automated (and do it!).&lt;/p&gt;

&lt;p&gt;There are a number of packages that allow you to schedule R scripts at specific times; two that come to mind are &lt;a href=&#34;https://cran.r-project.org/package=taskscheduleR&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;taskscheduleR&lt;/code&gt;&lt;/a&gt; (for Windows) and &lt;a href=&#34;https://cran.r-project.org/package=cronR&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;cronR&lt;/code&gt;&lt;/a&gt; (for Unix/Linux). We have &lt;code&gt;cronR&lt;/code&gt; setup on RStudio server at work, and it has a nice &lt;em&gt;RStudio addin&lt;/em&gt; which allows us to schedule tasks around any complex schedule. You can use these kind of tools &lt;a href=&#34;http://www.r-datacollection.com/blog/Welcome-to-the-ADCR-Blog/&#34; target=&#34;_blank&#34;&gt;to automate data collections&lt;/a&gt;, &lt;a href=&#34;http://www.r-datacollection.com/blog/Welcome-to-the-ADCR-Blog/&#34; target=&#34;_blank&#34;&gt;automate markdown reports to e-mail&lt;/a&gt;, &lt;a href=&#34;http://www.r-datacollection.com/blog/Welcome-to-the-ADCR-Blog/&#34; target=&#34;_blank&#34;&gt;or even the weather&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Alternatively, when you want a function (or script) to launch every time you start &lt;code&gt;R&lt;/code&gt; you can place these inside &lt;code&gt;.Rprofile&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;But what about if you want a function to start only the first time a user logs in? This could be something simple (like a reminder) to fill in a progress report, or in this case a bit of &lt;em&gt;Monday Morning (de)Motivation&lt;/em&gt;. This is a job for logfiles!&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s be straight here, I love Mondays (because &lt;a href=&#34;https://rweekly.org/&#34; target=&#34;_blank&#34;&gt;rweekly&lt;/a&gt; is released!), but most people don&amp;rsquo;t! Therefore, I made a &lt;code&gt;.csv&lt;/code&gt; file of &lt;code&gt;178&lt;/code&gt; quotes about Monday and a folder of 83 &lt;code&gt;.png&lt;/code&gt; images of cartoon characters sleeping. I used the &lt;code&gt;.First()&lt;/code&gt; function in &lt;code&gt;.Rprofile&lt;/code&gt; to create a logfile (if one doesn&amp;rsquo;t already exist) and randomly pair an image with a quote the first time a user logs in on Monday morning (using the &lt;code&gt;advicegiveR&lt;/code&gt; package).
Here&amp;rsquo;s some examples of images a user might see first thing Monday morning:&lt;/p&gt;

&lt;h3 id=&#34;the-good&#34;&gt;The Good&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;fig1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;the-bad&#34;&gt;The Bad&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;fig2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;the-ugly&#34;&gt;The Ugly&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;fig3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Any other time of that day (or any other day of the week) it will simply call a sage piece of collected advice from the &lt;code&gt;R-help&lt;/code&gt; forum via the &lt;code&gt;fortunes&lt;/code&gt; package. An example:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;R will always be arcane to those who do not make a serious effort to learn it. It is &lt;strong&gt;not&lt;/strong&gt; meant to be
intuitive and easy for casual users to just plunge into. It is far too complex and powerful for that. But
the rewards are great for serious data analysts who put in the effort.
   &amp;ndash; Berton Gunter
      R-help (August 2007)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You need to create a folder in your home directory that contains a &lt;code&gt;quotes.csv&lt;/code&gt;, a &lt;code&gt;LogFile.txt&lt;/code&gt; and &lt;code&gt;images&lt;/code&gt; folder.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;.First &amp;lt;- function(){
        today &amp;lt;- as.Date(Sys.Date())
        LastLog &amp;lt;- &amp;quot;&amp;quot;
        if(file.exists(&amp;quot;~/iHateMondays/LogFile.txt&amp;quot;)) {
                LogFile &amp;lt;- file(&amp;quot;~/iHateMondays/LogFile.txt&amp;quot;, open=&amp;quot;r&amp;quot;)
                LastLog &amp;lt;- readLines(LogFile, 1L)
                close(LogFile)
        }
        LogFile &amp;lt;- file(&amp;quot;~/iHateMondays/LogFile.txt&amp;quot;, open=&amp;quot;w&amp;quot;)
        writeLines(as.character(today), LogFile)
        close(LogFile)
        
        if(LastLog == as.character(today)) {
                # Already logged on today, just exit
                return()
        }
        
        ## If you get here, Need to run the first login code
        DOW &amp;lt;- weekdays(today)
        if (DOW == &amp;quot;Monday&amp;quot;) {
                # I Hate Mondays!
                suppressWarnings(library(advicegiveR, warn.conflicts = FALSE))
                suppressWarnings(library(readr, warn.conflicts = FALSE))
                quotes &amp;lt;- read_csv(&amp;quot;~/iHateMondays/quotes.csv&amp;quot;, col_names = FALSE)
                x1 &amp;lt;- sample(1:nrow(quotes), 1)
                advice &amp;lt;- as.character(quotes[x1,])
                y &amp;lt;- list.files(&amp;quot;~/iHateMondays/images&amp;quot;)
                # select one randomly
                y1 &amp;lt;- sample(1:length(y), 1)
                image &amp;lt;- magick::image_read(paste0(&amp;quot;~/iHateMondays/images&amp;quot;, y[y1]))
                # print image
                advicegiveR::print_advice(image = image, advice = advice, textcolor = &amp;quot;yellow&amp;quot;, size = 40)
        } else {
                # Fortune
                suppressWarnings(library(fortunes, warn.conflicts = FALSE))
                rnum &amp;lt;- sample(1:386, 1)
                print(fortune(rnum))
        }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now Ideally, you should use a contrasting border around the inner text color to make it readable on any background.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;fig4.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/leeper/meme&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;meme&lt;/code&gt; R package&lt;/a&gt; package uses a black border around text and positions text like a typical meme (top &amp;amp; bottom).&lt;/p&gt;

&lt;p&gt;By including a simple helper function to split the quote into equal lengthed top/bottom sections, the code would look like this:&lt;/p&gt;

&lt;style&gt;
pre.bluecars {
    background-color: #aabbff !important;
}
pre.redcars {
    background-color: #ffbbbb !important;
}
&lt;/style&gt;

&lt;pre&gt;&lt;code class=&#34;language-bluecars&#34;&gt;suppressWarnings(library(tidyverse))
word_split &amp;lt;- function(x, side=&amp;quot;left&amp;quot;, sep=&amp;quot; &amp;quot;) {
  words &amp;lt;- strsplit(as.character(x), sep)
  nwords &amp;lt;- lengths(words)
  if(side==&amp;quot;left&amp;quot;) {
    start &amp;lt;- 1
    end &amp;lt;- ceiling(nwords/2)
  } else if (side==&amp;quot;right&amp;quot;) {
    start &amp;lt;- ceiling((nwords+2)/2)
    end &amp;lt;- nwords
  }
  cw &amp;lt;- function(words, start, stop) paste(words[start:stop], collapse=sep)
  pmap_chr(list(words, start, end), cw)
}
left_words &amp;lt;- function(..., side) word_split(..., side=&amp;quot;left&amp;quot;)
right_words &amp;lt;- function(..., side) word_split(..., side=&amp;quot;right&amp;quot;)

.First &amp;lt;- function(){
        today &amp;lt;- as.Date(Sys.Date())
        LastLog &amp;lt;- &amp;quot;&amp;quot;
        if(file.exists(&amp;quot;~/iHateMondays/LogFile.txt&amp;quot;)) {
                LogFile &amp;lt;- file(&amp;quot;~/iHateMondays/LogFile.txt&amp;quot;, open=&amp;quot;r&amp;quot;)
                LastLog &amp;lt;- readLines(LogFile, 1L)
                close(LogFile)
        }
        LogFile &amp;lt;- file(&amp;quot;~/iHateMondays/LogFile.txt&amp;quot;, open=&amp;quot;w&amp;quot;)
        writeLines(as.character(today), LogFile)
        close(LogFile)

        if(LastLog == as.character(today)) {
                # Already logged on today, just exit
                return()
        }

        ## If you get here, Need to run the first login code
        DOW &amp;lt;- weekdays(today)
        if (DOW == &amp;quot;Monday&amp;quot;) {
                # I Hate Mondays!
                suppressWarnings(library(readr, warn.conflicts = FALSE))
                suppressWarnings(library(meme, warn.conflicts = FALSE))
                quotes &amp;lt;- read_csv(&amp;quot;~/iHateMondays/quotes.csv&amp;quot;, col_names = FALSE)
                x1 &amp;lt;- sample(1:nrow(quotes), 1)
                advice &amp;lt;- as.character(quotes[x1,])
                top_words &amp;lt;- left_words(advice)
                bottom_words &amp;lt;- right_words(advice)
                y &amp;lt;- list.files(&amp;quot;~/iHateMondays/images&amp;quot;)
                # select one randomly
                y1 &amp;lt;- sample(1:length(y), 1)
                image &amp;lt;- magick::image_read(paste0(&amp;quot;~/iHateMondays/images/&amp;quot;, y[y1]))
                # print image
                meme(paste0(&amp;quot;~/iHateMondays/images/&amp;quot;, y[y1]), size = &amp;quot;1.5&amp;quot;, upper = top_words, lower = bottom_words, color = &amp;quot;yellow&amp;quot;, font = &amp;quot;Times&amp;quot;)

        } else {
                # Fortune
                suppressWarnings(library(fortunes, warn.conflicts = FALSE))
                rnum &amp;lt;- sample(1:386, 1)
                print(fortune(rnum))
        }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;fig5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Image-grid with Ninjutsu CSS theme in Xaringan</title>
      <link>/post/ninjutsu/</link>
      <pubDate>Thu, 07 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/ninjutsu/</guid>
      <description>&lt;p&gt;A graduate student in our laboratory recently asked me if it was possible to include more than two images, side-by-side, in &lt;code&gt;Xaringan&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Of course anything is possible, however, most blog posts only touch on the most basic things you can do. With custom &lt;code&gt;CSS&lt;/code&gt; you can do whatever you want. I wanted to create an image-grid of a bunch of GIFs so I used the &lt;code&gt;ninjutsu&lt;/code&gt; theme to accomplish it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/4EFfHjHbgUnhzeZtg0/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://moldach.github.io/xaringan-presentation_ninjutsu/&#34; target=&#34;_blank&#34;&gt;Link to the slide-show&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Getting Started with Drake in R</title>
      <link>/post/drake/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/drake/</guid>
      <description>&lt;p&gt;Recently I got interested in using the &lt;code&gt;xaringan&lt;/code&gt; package for creating HTML based R Markdown documents because I could use the &lt;code&gt;widgetframe&lt;/code&gt; package to embed &lt;code&gt;htmlwidgets&lt;/code&gt; as responsive iframes (I had created some spinning 3D visulizations with pseudo-coloring I wanted to present interactively).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;xaringan&lt;/code&gt;, it&amp;rsquo;s a &lt;code&gt;R&lt;/code&gt; pacakges for creating slideshows with &lt;a href=&#34;http://remarkjs.com/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;remark.js&lt;/code&gt;&lt;/a&gt; through R Markdown.&lt;/p&gt;

&lt;p&gt;Around the same time I learnt about &lt;a href=&#34;https://github.com/ropensci/drake&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;Drake&lt;/code&gt;&lt;/a&gt; (or &amp;ldquo;Data Frames in R for Make&amp;rdquo;) which is an amazing a time-saving reproducible build system for data scientist specifically tailored to &lt;code&gt;R&lt;/code&gt;. The pacakge makes use of the &lt;code&gt;futures&lt;/code&gt; package allowing the use of parallel computing on high-performance computing systems.&lt;/p&gt;

&lt;p&gt;I recently gave a talk at the Douglas Mental Health University Institute talking about the advantages of Drake (and a few other interesting projects).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/3fhu1Pm9kYB5RgDtn7/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The slides can be found &lt;a href=&#34;https://moldach.github.io/xaringan-presentation_drake/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Web Scraping Google Sheets with RSelenium</title>
      <link>/project/rselenium/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 -0500</pubDate>
      
      <guid>/project/rselenium/</guid>
      <description></description>
    </item>
    
    <item>
      <title>10 Tips for Choosing the Optimal Number of Clusters</title>
      <link>/project/optimal-clustering/</link>
      <pubDate>Wed, 30 Jan 2019 00:00:00 -0500</pubDate>
      
      <guid>/project/optimal-clustering/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Gamification of Fitbit: How An API Provided The Next Level of tRaining</title>
      <link>/project/fitbit-project/</link>
      <pubDate>Sat, 10 Nov 2018 00:00:00 -0500</pubDate>
      
      <guid>/project/fitbit-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How to Highlight 3D Brain Regions</title>
      <link>/project/3d-brain/</link>
      <pubDate>Wed, 31 Oct 2018 00:00:00 -0400</pubDate>
      
      <guid>/project/3d-brain/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Transcriptome Assembly and Annotation</title>
      <link>/project/assembly/</link>
      <pubDate>Wed, 31 Oct 2018 00:00:00 -0400</pubDate>
      
      <guid>/project/assembly/</guid>
      <description></description>
    </item>
    
    <item>
      <title>vapoRwave ggplot2 themes &amp; palettes</title>
      <link>/project/vaporwave/</link>
      <pubDate>Wed, 31 Oct 2018 00:00:00 -0400</pubDate>
      
      <guid>/project/vaporwave/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How to Choose the Best Open Source Software</title>
      <link>/project/software-solutions/</link>
      <pubDate>Sat, 06 Oct 2018 00:00:00 -0400</pubDate>
      
      <guid>/project/software-solutions/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reversing the order of axis in a ggplot2 scatterplot</title>
      <link>/project/reverse-axis/</link>
      <pubDate>Sat, 06 Oct 2018 00:00:00 -0400</pubDate>
      
      <guid>/project/reverse-axis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Gold Standard for Data Science Project Management</title>
      <link>/project/project-management/</link>
      <pubDate>Sat, 06 Oct 2018 00:00:00 -0400</pubDate>
      
      <guid>/project/project-management/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 -0400</pubDate>
      
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
