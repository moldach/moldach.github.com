[{"authors":null,"categories":null,"content":"","date":1615273200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615273200,"objectID":"e8073464175e6c874a0d6b2d25c793ba","permalink":"/project/supervised-umap/","publishdate":"2021-03-09T00:00:00-07:00","relpermalink":"/project/supervised-umap/","section":"project","summary":"Using Uniform Manifold Approximation and Projection (UMAP) for supervised clustering and embedding over the similarity space computed from the leaves of a random forest.","tags":["R","Python","clustering","random forest","UMAP"],"title":"UMAP Supervised Embeddings (Python-\u003eR)","type":"project"},{"authors":null,"categories":[],"content":"\rCLICK HERE TO GO TO THE SHINY APPLICATION!!!\nInspiration for the Shiny Application\r\rYou can read as much theory as you want but the best way to learn something is to pull-up your sleeves and get your hands dirty by implementing a project.\n\rI‚Äôve been wanting to develop a Shiny application for sometime but the opportunity has never arisen. In my current role as a R developer, I provide client-side implementations via Java with the help of the RServe üì¶ - so Shiny isn‚Äôt needed. Well, if the opportunity doesn‚Äôt present itself on the job it‚Äôs time for a side-project!\nI wasn‚Äôt exactly sure what kind of project I should do until I saw a blog post by Timo Grossenbacher on bivariate maps with ggplot2 and sf.\nThe map shows income (in-)equality in Switzerland at the municipality level by visualizing two variables at the same time. What instantly attracted me was the beautiful relief of the mountainous Swiss landscape. It‚Äôs not something I‚Äôve seen much from ggplot2 maps - something usually reserved for GIS programs. I thought creating an application for geography would be a fun and engaging use of a Shiny application. My current role as a developer for a web-application that enables complex meta-analysis and visualization caters to biologists rather than specilazied bioinformaticians- NetworkAnalyst.ca. I thought it might be cool to do a similar thing for people who aren‚Äôt GIS specialist (or even familiar with R) and enable them to quickly create maps and save them (either as PNGs or PDFs)?\nOne of the intial challenges was finding a good dataset. I had wanted to do a project on my home province of Alberta but eventually settled on Hawaii instead for a couple reasons. One is that I could find relief‚Äôs for the State of Hawaii in two levels of detail: 100m and 200m. The second was the State of Hawaii also provides a large number of shapefiles which I could lay overtop of the relief. The amount of open data available to Hawaii attractive, and the fact that they are islands presented certain opportunities‚Ä¶ and challenges, as well.\nI wanted to provide users with a range maps. I wanted maps at different scales and with different contexts such as focusing on the marine environment or the terrestrial enviornment. I wanted maps covering the whole state of Hawaii, others showing individual islands (or groups of islands), and others giving the user a view of the city level. It was here that I got inspiration from Erin Davis‚Äôs beautiful street maps in ‚ÄúThe Beatiful Hidden Logic of Cities - Worldwide‚Äù.\nErin‚Äôs legend looked great! Unfortunately she didn‚Äôt have a programatic way to generate these labels, she did them in Adobe Photoshop (personal correspondence). I tried to write some code to generate labels programattically using key_glyph = \"point\" to set sf object glyphs as points and then use the override.aes trick to get circles (like in this SO post post) but I didn‚Äôt like the look. To be honest lables are something that could still use work in the ggplot2 world IMO ü§∑\nJake Kaupp did a #TidyTuesday submission for Week #40: Pizza Party! showing all of the pizza spots in the five-boroughs and came up with a nice way of constructing a color legend with ggplotGrob() as well as a neat use of colorspace::darken():\nFor Shiny UI inspiration I looked towards some of the winners of the 1st ever Shiny Contest (something I hope RStudio brings back next year!). I can‚Äôt express how much I learn from reading other peoples code and for this particular application I was heavily inspired by the David Smale‚Äôs, winner of ‚ÄúBest Design‚Äù, 69 Love Songs: A Lyrical Analysis. What was really cool was it was hosted on the RStudio cloud so I could go in and change things and see how it affected which for me was one of the coolest ways to grok Shiny.\n\rBuilding the application\rGetting back to elevation reliefs for a second. Before I learned that you could easily get Digital Elevation Model (DEM) data from mapzen via the {`geoviz‚Äô} üì¶, I used QGIS to subset the reliefs. Here‚Äôs a screencast of the process:\n\rFor the shapefiles, sometimes it was simple to subset the geom_sf() object via a column with island information (Hawaii, Maui, Oahu, Kauai) using dplyr::filter(). However, when there was no information I also used QGIS to subset the shapefiles.\n\rNote: I‚Äôm positive there is a way to do this via a bounding-box in R I just never learned how‚Ä¶ please send me a message and I‚Äôll update this if you know a way! Here‚Äôs a video of how I did it with QGIS:\n\r\rThis project was a great learning experience not only for learning Shiny \u0026amp; Spatial Mapping, but also learning a bit more about the development cycle of a ‚Äúproduction‚Äù applcation is like. It‚Äôs kind of like the ol‚Äô aphorism ‚ÄúPerfect is the eney of good‚Äù - you want to get your appltion out to the public ASAP. I released the app as soon as I had tested for bugs and thought the UI was good-enough.\nAfter its release, I wanted to add a new feature - Rayshader!, Rayshader is one of the hot-new tools in The Landscape of Spatial Data Analysis in R. Historically geographic information had always been presented in the form of two-dimensional maps, all the way from cave walls, parchment to computer screens. Because spatial information is inherently 3D, it makes sense that the tools available today are allowing 3D depictions of geographical data to escape the realm of mere novelty. With Rayshader you can control things like the lighting, camera tilt, and angle of view which provides an engaging experience for the audience.\nI was signed up for Tyler Morgan-Wall‚Äôs (the developer of Rayshader) Penn State MUSA Masterclass: 3D Mapping and Visualization with R and Rayshader the next month (the awesome talk is here and the Git repo is here) and wanted to spend a bit of time to demo the package before taking the class.\n\rI came up with some very plain ‚Äúdesert‚Äù-y looking maps of Hawaii:\n\rAlthough I had wanted to allow the user to interact with the 3D maps, I ran into issues where the application attempted to use more memory then was avialable under the free-tier plan of Shinyapps.io. The RStudio documentation on debugging your application‚Äôs section on Memory wa useful for figuring this out as it suggests looking in the application‚Äôs logs for 2019-11-18T15:12:16.088451+00:00 shinyapps[system]: Out of memory!. I settled on providing the ‚Äúfly-by‚Äù videos innstead.\nAfter attending Tyler‚Äôs class I wanted to update my application again by including a satellite overlay; I spent some time figuring out the best way to do this, modifying the easing function and rayshader::render_movie() parameters to make longer videos, testing it out, and then using rsconnect::deployApp() to re-deploy my application.\nI decided to merge all four videos together using VSCD Video Editor and overlaid some a musical track. Although Tyler promised Rayshader would have VR/AR capabilities in the near-future, I simply couldn‚Äôt wait so I also made another version of the video with a steroscopic 3D effect for viewing with red and cyan anaglyph 3D glasses:\n\rFuture Directions/Things Learned/General Musings\rI‚Äôm not planning on developing this specific application any further as I‚Äôm quite happy with the final outcome. This project was mainly to learn:\n\rShiny\rSpatial Mapping\rSoftware release life cycles\r\rFrom talking with my father he told me that software he worked on with IBM would be released with ~2000 known bugs.\n\r‚ÄúBetter a diamond with a flaw than a pebble without‚Äù - Confucius\n\rIt‚Äôs important to keep track of tasks, enhancements, and bugs for your project:\nNote: Those scrupulous enough to peer into my repo will have found that I do not always follow my own advice‚Ä¶\nNow theoretically let‚Äôs say I was building this application for a client and they just had to have! interactive 3D visualizations produced with Rayshader (something I‚Äôm not sure if Shiny is ideal for anyways) you could pay for a higher tier in Shinyapps.io to get a performance boost (i.e. more RAM üêè).\nAnother thing that I may have wanted to do if my application was more complex is to provide the user with a tour using the {cicerone} üì¶ like what one sees on the excellent tour of the iSEE Shiny interface.\nAfter 75 commits I‚Äôm pretty tired, so I didn‚Äôt include any code snippets here (BTW you can find all the code on Github). Full-disclosure: the 75 commtis (and code) are currently split between two-repositories and I feel a bit of shame for that‚Ä¶\nBut there is a reason for this. I had intially started off the [mahalo] repository because I wanted to create package like the {bcmaps} üì¶ but for Hawaii (I may still if there is enough interest from the community, but at the moment this is on-hold).\nAlong with this, I also wanted to create a Showcase/Companion for the package with Shiny that would give users a run-through.\nI wasn‚Äôt sure what the ‚Äúbest practices‚Äù for this are: do you keep the app with the package or do you have a seperate repo? I reached out to John Coene ([@jdatap](https://twitter.com/jdatap)), because he‚Äôs an all-around nice guy (and also Swiss), and he suggested keeping them seperate if the application is large (adivce I followed) and to also use {golem} (advice which I did not follow - I still think it‚Äôs a great idea and I‚Äôll consider for my next app though!).\nI still need to spend some time figuring out how to supply datasets as I ran into issues with my project. I knew that I wouldn‚Äôt be able to just drop all of the shapefiles into my git repo because Github is meant for code and not data. I also knew that you could acclerate ggplot2 by using a canvas to speed up rendering plots so I thought to myself ‚ÄúIf I‚Äôm just developing a map package I can generate‚Äùbasemaps\" as .Rds objects and then use those in my hinuhin repository to overlay other shapefiles on-top of\". Even though I avoided including most of the GIS information by producing these basemaps I had to set up a free account with Git Large File Storage to include the shapefiles I overlaid in this project and that took up 80% of my quota.\nOne way I had heard of dealing with this was either by hosting on a server: either, an internal server (means the Shiny application can only be used within an organization and not the general public) or hosting the data on the cloud (e.g. Amazon S3). Still other packages have dealt with this issue by accessing web-hosted databases via HTTP requests to APIs - which is well and good if the data is already available from an online database (it was not)\nThe bcmaps package handels data in a way proposed by G. Brooke Anderson \u0026amp; Dirk Eddelbuettel ([@eddelbuettel](https://twitter.com/eddelbuettel)), via a drat repository which allows a recommended maximum size of 1GB - I may have to look into this option if the Twitterverse is interested in a package for maps of Hawaii.\n\rBrief thought on the future of Rayshader\rThere are a number of companies today trying to map out city streets to create high-quality 3D maps for self-driving cars and projection projects (see: The Race to Map the World in 3D):\nImage: Carmera\n\rGetting access to this kind of data would be awesome because it would mean you could overlay high-quality images over LIDAR like this one produced by Tyler of Philadelphia:\n\rIf you missed it at the top you can CLICK HERE TO GO TO MY SHINY APPLICATION DEPLOYMENT!!!\n\r\r\r","date":1574467200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574467200,"objectID":"97b3031e33a83387662da6e9dcd73930","permalink":"/project/taster/","publishdate":"2019-11-23T00:00:00Z","relpermalink":"/project/taster/","section":"project","summary":"CLICK HERE TO GO TO THE SHINY APPLICATION!!!\nInspiration for the Shiny Application\r\rYou can read as much theory as you want but the best way to learn something is to pull-up your sleeves and get your hands dirty by implementing a project.\n\rI‚Äôve been wanting to develop a Shiny application for sometime but the opportunity has never arisen. In my current role as a R developer, I provide client-side implementations via Java with the help of the RServe üì¶ - so Shiny isn‚Äôt needed.","tags":[],"title":"Shiny Dev and Software Release Cycles and Rayshader, Oh My!","type":"project"},{"authors":["Matthew J. Oldach"],"categories":[],"content":" Volcom was founded in 1991, it was the first company to combine skateboarding, surfing and snowboarding under one brand from it\u0026rsquo;s incepetion. I always loved how their aesthethics captured the energy and artistry of board-riding in its purest form.\nIf you have no idea what I\u0026rsquo;m talking about here\u0026rsquo;s some samples from print advertisments and a clip from 2003\u0026rsquo;s Big Youth Happening which I would say was the ultimate inspiration for this post.\nNon-photorealistic rendering (NPR) is a combination of computer graphics and computer vision that produces renderings in various artistic, expressive or stylized ways. A new art form that couldn\u0026rsquo;t have existed without computers. Researchers have proposed many algorithms and styles such as painting, pen-and-ink drawing, tile mosaics, stippling, streamline visualization, and tensor field visualization. Although the details vary, the goal is to make an image look like some other image. Two main approaches to designing these algorithms exist. Greedy algorithms greedily place strokes to match the target goas. Optimization algorithms iteratively place and then adjust stroke positions to minimize the objective function. NPR can be done using a number of machine learning methods, most commonly General Adverserial Networks or Convolution Neural Networks(CNN) such as painting and drawing.\nI wanted to use lengstrom/fast-style-transfer, an optimization technique, on a video of my friends and I backcountry skiing and snowboarding in The Rocky Mountains of Alberta.\nFast-style-transfer is a TensorFlow CNN model based on a combination of Gatys\u0026rsquo; A Neural Algorithm of Artistic Style, Johnson\u0026rsquo;s Perceptual Losses for Real-Time Style Transfer and Super-Resolution, and Ulyanov\u0026rsquo;s Instance Normalization. I\u0026rsquo;ll skip the details here because it\u0026rsquo;s been been far better described in the publications above (or in layman\u0026rsquo;s terms here, here, and here), suffice it to say it allows you to compose images/videos in the style of another image. Besides being fun it\u0026rsquo;s a nice visual way of showing the capabilities and internal representations of neural networks.\nfast-style-transfer repo comes with 6 trained models on famous paintings, allowing you to quikcly style your images (or videos) like them. But what\u0026rsquo;s the fun in doing what others have alread done?\nI wanted to see what kind of effect would be produced by training style transfer networks around interesting shapes and patterns! I selected a number patterns I thought would be interesting, for example animal patterns, fractals, fibonnaci sequences, a few vaporwave images (of course), and a bunch of images taken under a SEM microscope.\nI then constructed a script to train 50 models simultaneously on a HPC (using a GPU NVIDIA V100 Volta; a top-of-the-line $10,000 GPU - spared no expense). After many many hours of training, across hundreds of cores, I selected the 20 best (i.e. most aesthetically pleasing) models to apply to the video.\n(The entire high-quality video can be found here](https://youtu.be/TuN4456PK-c).\nTechnical Notes: When training the models I noticed that the checkpoint files were being corrupted. I found a closed issue on the repo: #78 stating that Tensorflow\u0026rsquo;s Saver class was updated so you\u0026rsquo;ll need to change line 136 in /src/optimize.py.\nThe script for training all of the networks on an HPC with a SLURM job scheduler is below, and can be run with bash fastTrainer.bash \u0026lt;image_path\u0026gt; \u0026lt;out_path\u0026gt; \u0026lt;test_path\u0026gt;. Simply clone lengstrom/fast-style-transfer and create three folders inside. Create a sub-directory where you\u0026rsquo;ll put all the style images you want to train, another sub-directory for the output (.ckpt files used to style your .mp4 videos), and a test sub-directory for the model. As a default I used the chicago for training and I\u0026rsquo;m not entirely sure if that makes a difference.\n!/bin/bash #$ -pwd # bash fastTrainer.bash /images/ /outpath/ /testpath/ ## ## An embarrassingly parallel script to train many style transfer networks on a HPC ## Access to SLURM job scheduler and fast-style-transfer is required to run this program. ## The three mandatory pathways must be specified in the indicated order. IMG=$(readlink -f \u0026quot;${1%/}\u0026quot;) # path_to_train_images OUT_DIR=$(readlink -f \u0026quot;${2%/}\u0026quot;) # path_to_checkpoints TEST=$(readlink -f \u0026quot;${3%/}\u0026quot;) # path_to_tests mkdir -p ${OUT_DIR}/jobs JID=0 # job ID for SLURM job name for f in ${IMG}/*; do let JID=(JID+1) cat \u0026gt; ${OUT_DIR}/jobs/style_${JID}.bash \u0026lt;\u0026lt; EOT # write job information for each job #!/bin/bash #SBATCH --gres=gpu:1 # request GPU #SBATCH --cpus-per-task=2 # maximum CPU cores per GPU request #SBATCH --time=00:01:00 # request 8 hours of walltime #SBATCH --job-name=\u0026quot;fst_${JID}\u0026quot; #SBATCH --output=${OUT_DIR}/jobs/%N-%j.out # %N for node name, %j for jobID ### JOB SCRIPT BELLOW ### # Load Modules module load python/2.7.14 module load scipy-stack source tensorflow/bin/activate mkdir ${OUT_DIR}/${JID} mkdir ${TEST}/${JID} python style.py --style $f \\ --checkpoint-dir ${OUT_DIR}/${JID} \\ --test examples/content/chicago.jpg \\ --test-dir ${OUT_DIR}/${JID} \\ --content-weight 1.5e1 \\ --checkpoint-iterations 1000 \\ --batch-size 20 EOT chmod 754 $(readlink -f \u0026quot;${OUT_DIR}\u0026quot;)/jobs/style_${JID}.bash sbatch $(readlink -f \u0026quot;${OUT_DIR}\u0026quot;)/jobs/style_${JID}.bash done  What I learned Now Sean Dorrance Kelly might argue I\u0026rsquo;m using the machines \u0026ldquo;creativity\u0026rdquo; as a subsititute for my own but I would disagree since there was a fair amount of creativity in selecting images to train models on and the editing itself. That being said an AI generated work of art recently sold for $432,500 at Christie\u0026rsquo;s (the first auction house to offer a piece of art created by an algorithm).\nBesides the opportunity to exercise my right-brain, it was a good chance to gain some experience with tensorflow and python. This project was also illuminating because it made the hidden layers of networks more comprehensible by literally allowing me to see how neural networks are performing!\nI think the field of Feature visualization is exciting because it allows to peer behind the curtain and see how networks learn to classify images accurately. Take for example the Activation atlas which reveals visual abstractions within a model. It gives a global view of a dataset by showing feature visualization of averaged activation values from a neural network.\n As an analogy, while the 26 letters in the alphabet provide a basis for English, seeing how letters are commonly combined to make words gives far more insight into the concepts that can be expressed than the letters alone. Similarly, activation atlases give us a bigger picture view by showing common combinations of neurons.\n In a perfect world the artist/user should have control over the decisions made by the algorithm. For example, to specify spatially varying styles to use, so that different rendering styles are used in different parts of the image, or to specify positions of individuial strokes. However, training these models is still too slow to be useful in an interactive application.\n","date":1555804800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555804800,"objectID":"4c9587f5d30f42fac519d583f09a80f1","permalink":"/post/faststyletransfer/","publishdate":"2019-04-21T00:00:00Z","relpermalink":"/post/faststyletransfer/","section":"post","summary":"Volcom was founded in 1991, it was the first company to combine skateboarding, surfing and snowboarding under one brand from it\u0026rsquo;s incepetion. I always loved how their aesthethics captured the energy and artistry of board-riding in its purest form.\nIf you have no idea what I\u0026rsquo;m talking about here\u0026rsquo;s some samples from print advertisments and a clip from 2003\u0026rsquo;s Big Youth Happening which I would say was the ultimate inspiration for this post.","tags":[],"title":"TensorFlow CNN for Fast Style Transfer","type":"post"},{"authors":["Matthew J. Oldach"],"categories":[],"content":" Just as test automation is a critical part of rutheless testing, your goal for any project should be to identify taks that can be automated (and do it!).\nThere are a number of packages that allow you to schedule R scripts at specific times; two that come to mind are taskscheduleR (for Windows) and cronR (for Unix/Linux). We have cronR setup on RStudio server at work, and it has a nice RStudio addin which allows us to schedule tasks around any complex schedule. You can use these kind of tools to automate data collections, automate markdown reports to e-mail, or even the weather.\nAlternatively, when you want a function (or script) to launch every time you start R you can place these inside .Rprofile.\nBut what about if you want a function to start only the first time a user logs in? This could be something simple (like a reminder) to fill in a progress report, or in this case a bit of Monday Morning (de)Motivation. This is a job for logfiles!\nLet\u0026rsquo;s be straight here, I love Mondays (because rweekly is released!), but most people don\u0026rsquo;t! Therefore, I made a .csv file of 178 quotes about Monday and a folder of 83 .png images of cartoon characters sleeping. I used the .First() function in .Rprofile to create a logfile (if one doesn\u0026rsquo;t already exist) and randomly pair an image with a quote the first time a user logs in on Monday morning (using the advicegiveR package). Here\u0026rsquo;s some examples of images a user might see first thing Monday morning:\nThe Good The Bad The Ugly Any other time of that day (or any other day of the week) it will simply call a sage piece of collected advice from the R-help forum via the fortunes package. An example:\n R will always be arcane to those who do not make a serious effort to learn it. It is not meant to be intuitive and easy for casual users to just plunge into. It is far too complex and powerful for that. But the rewards are great for serious data analysts who put in the effort. \u0026ndash; Berton Gunter R-help (August 2007)\n You need to create a folder in your home directory that contains a quotes.csv, a LogFile.txt and images folder.\n.First \u0026lt;- function(){ today \u0026lt;- as.Date(Sys.Date()) LastLog \u0026lt;- \u0026quot;\u0026quot; if(file.exists(\u0026quot;~/iHateMondays/LogFile.txt\u0026quot;)) { LogFile \u0026lt;- file(\u0026quot;~/iHateMondays/LogFile.txt\u0026quot;, open=\u0026quot;r\u0026quot;) LastLog \u0026lt;- readLines(LogFile, 1L) close(LogFile) } LogFile \u0026lt;- file(\u0026quot;~/iHateMondays/LogFile.txt\u0026quot;, open=\u0026quot;w\u0026quot;) writeLines(as.character(today), LogFile) close(LogFile) if(LastLog == as.character(today)) { # Already logged on today, just exit return() } ## If you get here, Need to run the first login code DOW \u0026lt;- weekdays(today) if (DOW == \u0026quot;Monday\u0026quot;) { # I Hate Mondays! suppressWarnings(library(advicegiveR, warn.conflicts = FALSE)) suppressWarnings(library(readr, warn.conflicts = FALSE)) quotes \u0026lt;- read_csv(\u0026quot;~/iHateMondays/quotes.csv\u0026quot;, col_names = FALSE) x1 \u0026lt;- sample(1:nrow(quotes), 1) advice \u0026lt;- as.character(quotes[x1,]) y \u0026lt;- list.files(\u0026quot;~/iHateMondays/images\u0026quot;) # select one randomly y1 \u0026lt;- sample(1:length(y), 1) image \u0026lt;- magick::image_read(paste0(\u0026quot;~/iHateMondays/images\u0026quot;, y[y1])) # print image advicegiveR::print_advice(image = image, advice = advice, textcolor = \u0026quot;yellow\u0026quot;, size = 40) } else { # Fortune suppressWarnings(library(fortunes, warn.conflicts = FALSE)) rnum \u0026lt;- sample(1:386, 1) print(fortune(rnum)) } }  Now Ideally, you should use a contrasting border around the inner text color to make it readable on any background.\nThe meme R package package uses a black border around text and positions text like a typical meme (top \u0026amp; bottom).\nBy including a simple helper function to split the quote into equal lengthed top/bottom sections, the code would look like this:\n pre.bluecars { background-color: #aabbff !important; } pre.redcars { background-color: #ffbbbb !important; }  suppressWarnings(library(tidyverse)) word_split \u0026lt;- function(x, side=\u0026quot;left\u0026quot;, sep=\u0026quot; \u0026quot;) { words \u0026lt;- strsplit(as.character(x), sep) nwords \u0026lt;- lengths(words) if(side==\u0026quot;left\u0026quot;) { start \u0026lt;- 1 end \u0026lt;- ceiling(nwords/2) } else if (side==\u0026quot;right\u0026quot;) { start \u0026lt;- ceiling((nwords+2)/2) end \u0026lt;- nwords } cw \u0026lt;- function(words, start, stop) paste(words[start:stop], collapse=sep) pmap_chr(list(words, start, end), cw) } left_words \u0026lt;- function(..., side) word_split(..., side=\u0026quot;left\u0026quot;) right_words \u0026lt;- function(..., side) word_split(..., side=\u0026quot;right\u0026quot;) .First \u0026lt;- function(){ today \u0026lt;- as.Date(Sys.Date()) LastLog \u0026lt;- \u0026quot;\u0026quot; if(file.exists(\u0026quot;~/iHateMondays/LogFile.txt\u0026quot;)) { LogFile \u0026lt;- file(\u0026quot;~/iHateMondays/LogFile.txt\u0026quot;, open=\u0026quot;r\u0026quot;) LastLog \u0026lt;- readLines(LogFile, 1L) close(LogFile) } LogFile \u0026lt;- file(\u0026quot;~/iHateMondays/LogFile.txt\u0026quot;, open=\u0026quot;w\u0026quot;) writeLines(as.character(today), LogFile) close(LogFile) if(LastLog == as.character(today)) { # Already logged on today, just exit return() } ## If you get here, Need to run the first login code DOW \u0026lt;- weekdays(today) if (DOW == \u0026quot;Monday\u0026quot;) { # I Hate Mondays! suppressWarnings(library(readr, warn.conflicts = FALSE)) suppressWarnings(library(meme, warn.conflicts = FALSE)) quotes \u0026lt;- read_csv(\u0026quot;~/iHateMondays/quotes.csv\u0026quot;, col_names = FALSE) x1 \u0026lt;- sample(1:nrow(quotes), 1) advice \u0026lt;- as.character(quotes[x1,]) top_words \u0026lt;- left_words(advice) bottom_words \u0026lt;- right_words(advice) y \u0026lt;- list.files(\u0026quot;~/iHateMondays/images\u0026quot;) # select one randomly y1 \u0026lt;- sample(1:length(y), 1) image \u0026lt;- magick::image_read(paste0(\u0026quot;~/iHateMondays/images/\u0026quot;, y[y1])) # print image meme(paste0(\u0026quot;~/iHateMondays/images/\u0026quot;, y[y1]), size = \u0026quot;1.5\u0026quot;, upper = top_words, lower = bottom_words, color = \u0026quot;yellow\u0026quot;, font = \u0026quot;Times\u0026quot;) } else { # Fortune suppressWarnings(library(fortunes, warn.conflicts = FALSE)) rnum \u0026lt;- sample(1:386, 1) print(fortune(rnum)) } }  ","date":1554163200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554163200,"objectID":"31f1aaa0b75abc4e94d1429a8a81fffd","permalink":"/post/rprofile/","publishdate":"2019-04-02T00:00:00Z","relpermalink":"/post/rprofile/","section":"post","summary":"Just as test automation is a critical part of rutheless testing, your goal for any project should be to identify taks that can be automated (and do it!).\nThere are a number of packages that allow you to schedule R scripts at specific times; two that come to mind are taskscheduleR (for Windows) and cronR (for Unix/Linux). We have cronR setup on RStudio server at work, and it has a nice RStudio addin which allows us to schedule tasks around any complex schedule.","tags":[],"title":"Step aside Cron, this is a job for .Rprofile \u0026 logfiles!","type":"post"},{"authors":["Matthew J. Oldach"],"categories":[],"content":"A graduate student in our laboratory recently asked me if it was possible to include more than two images, side-by-side, in Xaringan.\nOf course anything is possible, however, most blog posts only touch on the most basic things you can do. With custom CSS you can do whatever you want. I wanted to create an image-grid of a bunch of GIFs so I used the ninjutsu theme to accomplish it.\nLink to the slide-show\n","date":1551916800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551916800,"objectID":"2efca2fb9187e578af4186ab31b47cc0","permalink":"/post/ninjutsu/","publishdate":"2019-03-07T00:00:00Z","relpermalink":"/post/ninjutsu/","section":"post","summary":"A graduate student in our laboratory recently asked me if it was possible to include more than two images, side-by-side, in Xaringan.\nOf course anything is possible, however, most blog posts only touch on the most basic things you can do. With custom CSS you can do whatever you want. I wanted to create an image-grid of a bunch of GIFs so I used the ninjutsu theme to accomplish it.","tags":[],"title":"Image-grid with Ninjutsu CSS theme in Xaringan","type":"post"},{"authors":["Matthew J. Oldach"],"categories":[],"content":"Recently I got interested in using the xaringan package for creating HTML based R Markdown documents because I could use the widgetframe package to embed htmlwidgets as responsive iframes (I had created some spinning 3D visulizations with pseudo-coloring I wanted to present interactively).\nxaringan, it\u0026rsquo;s a R pacakges for creating slideshows with remark.js through R Markdown.\nAround the same time I learnt about Drake (or \u0026ldquo;Data Frames in R for Make\u0026rdquo;) which is an amazing a time-saving reproducible build system for data scientist specifically tailored to R. The pacakge makes use of the futures package allowing the use of parallel computing on high-performance computing systems.\nI recently gave a talk at the Douglas Mental Health University Institute talking about the advantages of Drake (and a few other interesting projects).\nThe slides can be found here\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"d020517875ffc995fcc285f4429ea0dc","permalink":"/post/drake/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/drake/","section":"post","summary":"Recently I got interested in using the xaringan package for creating HTML based R Markdown documents because I could use the widgetframe package to embed htmlwidgets as responsive iframes (I had created some spinning 3D visulizations with pseudo-coloring I wanted to present interactively).\nxaringan, it\u0026rsquo;s a R pacakges for creating slideshows with remark.js through R Markdown.\nAround the same time I learnt about Drake (or \u0026ldquo;Data Frames in R for Make\u0026rdquo;) which is an amazing a time-saving reproducible build system for data scientist specifically tailored to R.","tags":[],"title":"Getting Started with Drake in R","type":"post"},{"authors":null,"categories":null,"content":"","date":1548918000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548918000,"objectID":"828c433ee6b2db0f7e6e716e0ebea6b8","permalink":"/project/rselenium/","publishdate":"2019-01-31T00:00:00-07:00","relpermalink":"/project/rselenium/","section":"project","summary":"","tags":["R","Selenium","Web Scraping"],"title":"Web Scraping Google Sheets with RSelenium","type":"project"},{"authors":null,"categories":null,"content":"","date":1548831600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548831600,"objectID":"9509e359b6487091879d5b2a922ccaf3","permalink":"/project/optimal-clustering/","publishdate":"2019-01-30T00:00:00-07:00","relpermalink":"/project/optimal-clustering/","section":"project","summary":"","tags":["R","unsupervised learning","clustering","dataviz"],"title":"10 Tips for Choosing the Optimal Number of Clusters","type":"project"},{"authors":null,"categories":null,"content":"","date":1541833200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541833200,"objectID":"07638dd7098574f6d189725766120e37","permalink":"/project/fitbit-project/","publishdate":"2018-11-10T00:00:00-07:00","relpermalink":"/project/fitbit-project/","section":"project","summary":"Accessing biometric data from the Fitbit API","tags":["Demo"],"title":"The Gamification of Fitbit: How An API Provided The Next Level of tRaining","type":"project"},{"authors":null,"categories":null,"content":"","date":1540965600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540965600,"objectID":"33e2f22744f7ffc90b6608034faa6d64","permalink":"/project/3d-brain/","publishdate":"2018-10-31T00:00:00-06:00","relpermalink":"/project/3d-brain/","section":"project","summary":"A tutorial for three methods of labelling an ROI in brain images","tags":["R","MATLAB","dataviz"],"title":"How to Highlight 3D Brain Regions","type":"project"},{"authors":null,"categories":null,"content":"","date":1540965600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540965600,"objectID":"5c81a4c93facf0146bee152ce4445125","permalink":"/project/assembly/","publishdate":"2018-10-31T00:00:00-06:00","relpermalink":"/project/assembly/","section":"project","summary":"Transcriptome Assembly and Annotation.","tags":["R","MATLAB","dataviz"],"title":"Transcriptome Assembly and Annotation","type":"project"},{"authors":null,"categories":null,"content":"","date":1540965600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540965600,"objectID":"15b770d7945e057af25b70575d2a17d2","permalink":"/project/vaporwave/","publishdate":"2018-10-31T00:00:00-06:00","relpermalink":"/project/vaporwave/","section":"project","summary":"This package provides a number of ggplot2 themes inspired by vaporwave, both a subgenre of electronic music and an art movement.","tags":["R","MATLAB","dataviz"],"title":"vapoRwave ggplot2 themes \u0026 palettes","type":"project"},{"authors":null,"categories":null,"content":"","date":1538805600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538805600,"objectID":"10c51808100a791168172f28e1665396","permalink":"/project/software-solutions/","publishdate":"2018-10-06T00:00:00-06:00","relpermalink":"/project/software-solutions/","section":"project","summary":"How to Choose the Best Open Source Software","tags":["R","Linux"],"title":"How to Choose the Best Open Source Software","type":"project"},{"authors":null,"categories":null,"content":"","date":1538805600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538805600,"objectID":"ac51ee586d1e4b697548c53785cb3906","permalink":"/project/reverse-axis/","publishdate":"2018-10-06T00:00:00-06:00","relpermalink":"/project/reverse-axis/","section":"project","summary":"Playing around with the axis display in ggplot2","tags":["R","dataviz","ggplot2"],"title":"Reversing the order of axis in a ggplot2 scatterplot","type":"project"},{"authors":null,"categories":null,"content":"","date":1538805600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538805600,"objectID":"9d9c34ceb8b0c1a50201ee433eeea314","permalink":"/project/project-management/","publishdate":"2018-10-06T00:00:00-06:00","relpermalink":"/project/project-management/","section":"project","summary":"Gold Standard workflow for setting up a new data science project directory and file naming","tags":["R","Linux"],"title":"The Gold Standard for Data Science Project Management","type":"project"},{"authors":null,"categories":null,"content":"","date":1528264800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528264800,"objectID":"50849994ea21f115ab228a5a65e77f98","permalink":"/project/knowledge-is-beautiful/","publishdate":"2018-06-06T00:00:00-06:00","relpermalink":"/project/knowledge-is-beautiful/","section":"project","summary":"In this series I will set out to recreate some of the visualization from the book ‚ÄúKnowledge is Beautiful‚Äù by David McCandless in R.","tags":["Demo"],"title":"Recreating data visualizations from the book Knowledge is Beautiful","type":"project"},{"authors":null,"categories":null,"content":"\u0026lt;!DOCTYPE html\u0026gt;\n\n\nindex.utf8\n       code{white-space: pre-wrap;} span.smallcaps{font-variant: small-caps;} span.underline{text-decoration: underline;} div.column{display: inline-block; vertical-align: top; width: 50%;} div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;} ul.task-list{list-style: none;}  code{white-space: pre;} pre:not([class]) { background-color: white; }  if (window.hljs) { hljs.configure({languages: []}); hljs.initHighlightingOnLoad(); if (document.readyState \u0026\u0026 document.readyState === \"complete\") { window.setTimeout(function() { hljs.initHighlighting(); }, 0); } }  h1 { font-size: 34px; } h1.title { font-size: 38px; } h2 { font-size: 30px; } h3 { font-size: 24px; } h4 { font-size: 18px; } h5 { font-size: 16px; } h6 { font-size: 12px; } .table th:not([align]) { text-align: left; }  .main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.04); } img { max-width:100%; } .tabbed-pane { padding-top: 12px; } .html-widget { margin-bottom: 20px; } button.code-folding-btn:focus { outline: none; } summary { display: list-item; }  .tabset-dropdown  .nav-tabs { display: inline-table; max-height: 500px; min-height: 44px; overflow-y: auto; background: white; border: 1px solid #ddd; border-radius: 4px; } .tabset-dropdown  .nav-tabs  li.active:before { content: \"Óâô\"; font-family: 'Glyphicons Halflings'; display: inline-block; padding: 10px; border-right: 1px solid #ddd; } .tabset-dropdown  .nav-tabs.nav-tabs-open  li.active:before { content: \"Óâò\"; border: none; } .tabset-dropdown  .nav-tabs.nav-tabs-open:before { content: \"Óâô\"; font-family: 'Glyphicons Halflings'; display: inline-block; padding: 10px; border-right: 1px solid #ddd; } .tabset-dropdown  .nav-tabs  li.active { display: block; } .tabset-dropdown  .nav-tabs  li  a, .tabset-dropdown  .nav-tabs  li  a:focus, .tabset-dropdown  .nav-tabs  li  a:hover { border: none; display: inline-block; border-radius: 4px; background-color: transparent; } .tabset-dropdown  .nav-tabs.nav-tabs-open  li { display: block; float: none; } .tabset-dropdown  .nav-tabs  li { display: none; }  \n\n.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}  .parallax { /* The image used */ background-image: url(\"https://images.unsplash.com/photo-1578801503688-d30fffee6a9e?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D\u0026ixlib=rb-1.2.1\u0026auto=format\u0026fit=crop\u0026w=937\u0026q=80\"); /* Set a specific height */ height: 400px; /* Create the parallax scrolling effect */ background-attachment: fixed; background-position: center; background-repeat: no-repeat; background-size: cover; } .myimg{ width: 10%; color: red; position: absolute; top: 150px; left: 45%; }     UMAP Supervised Embeddings (Python-\u0026gt;R)    .python-code { background-color: #b55e49; }  .r-code { background-color: #4c5b64; }  \nProject Overview I‚Äôve transcompiled code from Python (code shown in red) to R(code shown in blue) in order to do supervised dimensionality reduction with Random Forests (RF) and UMAP following this blog post.\nThe goal was to get an array that contains the leaf indices that each sample was assigned to in the forest in order to feed this information into {uwot} (for UMAP implementation in R).\nI tried out the following R packages, {randomForest}, {ranger}, and {extraTrees}. {ranger} was expected to require the lowest compute-time (critical for my project) while {extraTrees}, in some-cases, can outperform RF implementations (in terms of AUC). _Note: For this project, we are providing a Java facing UI to clinicians who are Subject Matter Experts who have background knowledge in the underlying biology of an analysis but lack the computational expertise for such analysis. Also note, we are limited to 1 core on the Java server we provide for analysis; so, necessarily we will benchmark tools in serial below.\nIn Python, scikit-learn the ExtraTreesClassifier returns a numpy array which contains the leaf indices that each sample was assigned to in the forest like so:\nTo make it as comparable as-possible between R/Python we will do the brunt of the work using Python using {reticulate} following instructions from the blog (however, here, we are decreasing the size of simulated data) comparing only the various RF implementations.\n Setting up the Python environment reticulate::conda_create(\u0026#39;r-reticulate\u0026#39;, python_version = 3.6) PYTHON_DEPENDENCIES = c(\u0026#39;pandas\u0026#39;, \u0026#39;numpy\u0026#39;, \u0026#39;numba\u0026#39;, \u0026#39;scipy\u0026#39;, \u0026#39;matplotlib\u0026#39;, \u0026#39;scikit-learn\u0026#39;, \u0026#39;umap-learn\u0026#39;, \u0026#39;tqdm\u0026#39;, \u0026#39;pynndescent\u0026#39;, \u0026#39;fastcluster\u0026#39; ) reticulate::conda_install(\u0026#39;r-reticulate\u0026#39;, packages = PYTHON_DEPENDENCIES, ignore_installed=TRUE) Python Implementation import numpy as np import pandas as pd import scipy as sp import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.model_selection import cross_val_predict, StratifiedKFold from sklearn.metrics import roc_auc_score from sklearn.datasets import make_classification from tqdm import tqdm from umap import UMAP ## if this doesn\u0026#39;t work try: import umap.umap_ as UMAP from pynndescent import NNDescent from fastcluster import single from scipy.cluster.hierarchy import cut_tree, fcluster, dendrogram from scipy.spatial.distance import squareform from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier from sklearn.model_selection import train_test_split # turning off automatic plot showing, and setting style plt.style.use(\u0026#39;bmh\u0026#39;) # let us generate some data with 10 clusters per class X, y = make_classification(n_samples=10000, n_features=500, n_informative=5, n_redundant=0, n_clusters_per_class=10, weights=[0.80], flip_y=0.05, class_sep=3.5, random_state=42) # normalizing to eliminate scaling differences X = pd.DataFrame(StandardScaler().fit_transform(X)) # Split dataset into training set and test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # 70% training and 30% test Let‚Äôs use Python‚Äôs ExtraTreesClassifier to get the information from leaf indices and plot.\nIn the blog post the author used StratifiedKFold \u0026amp; cross_val_predict. Here I use train_test_split instead to create a train/test split. This way I can use the same train/split for Python \u0026amp; R and ensure the Area under the ROC curve measurement is comparable.\n# model instance et = ExtraTreesClassifier(n_estimators=100, min_samples_leaf=500, max_features=0.80, bootstrap=True, class_weight=\u0026#39;balanced\u0026#39;, n_jobs=-1, random_state=42) # Train ExtraTreesClassifer et.fit(X_train, y_train) # Print the AUC ## ExtraTreesClassifier(bootstrap=True, class_weight=\u0026#39;balanced\u0026#39;, max_features=0.8, ## min_samples_leaf=500, n_jobs=-1, random_state=42) print(\u0026#39;Area under the ROC Curve:\u0026#39;, roc_auc_score(y_test, et.predict_proba(X_test)[:,1])) ## Area under the ROC Curve: 0.850375887423935 The models performance is 0.8504 AUC. Now we move on to embedding.\n# let us train our model with the full data et.fit(X, y) # and get the leaves that each sample was assigned to ## ExtraTreesClassifier(bootstrap=True, class_weight=\u0026#39;balanced\u0026#39;, max_features=0.8, ## min_samples_leaf=500, n_jobs=-1, random_state=42) leaves = et.apply(X) What are dimensions of the numpy array?\nleaves.shape ## (10000, 100) # calculating the embedding with hamming distance sup_embed_et = UMAP(metric=\u0026#39;hamming\u0026#39;).fit_transform(leaves) # plotting the embedding ## /home/mtg/miniconda3/envs/r-reticulate/lib/python3.6/site-packages/umap/umap_.py:1762: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable ## \u0026quot;inverse_transform will be unavailable\u0026quot;.format(self.metric) plt.figure(figsize=(12,7), dpi=150) plt.scatter(sup_embed_et[y == 0,0], sup_embed_et[y == 0,1], s=1, c=\u0026#39;C0\u0026#39;, cmap=\u0026#39;viridis\u0026#39;, label=\u0026#39;$y=0$\u0026#39;) ## \u0026lt;matplotlib.collections.PathCollection object at 0x7f1b12fefc50\u0026gt; plt.scatter(sup_embed_et[y == 1,0], sup_embed_et[y == 1,1], s=1, c=\u0026#39;C1\u0026#39;, cmap=\u0026#39;viridis\u0026#39;, label=\u0026#39;$y=1$\u0026#39;) ## \u0026lt;matplotlib.collections.PathCollection object at 0x7f1b12d247f0\u0026gt; plt.title(\u0026#39;Supervised embedding with ExtraTrees\u0026#39;) ## Text(0.5, 1.0, \u0026#39;Supervised embedding with ExtraTrees\u0026#39;) plt.xlabel(\u0026#39;$x_0$\u0026#39;); plt.ylabel(\u0026#39;$x_1$\u0026#39;) ## Text(0.5, 0, \u0026#39;$x_0$\u0026#39;) ## Text(0, 0.5, \u0026#39;$x_1$\u0026#39;) plt.legend(fontsize=16, markerscale=5); plt.show() # taking a sample of the dataframe embed_sample = pd.DataFrame(sup_embed_et).sample(5000, random_state=42) # running fastcluster hierarchical clustering on the improved embedding H = single(embed_sample) # getting the clusters clusters = cut_tree(H, height=0.35) print(\u0026#39;Number of clusters:\u0026#39;, len(np.unique(clusters))) ## Number of clusters: 19 This shows 19 clusters (the simulated number was 20).\n# creating a dataframe for the clustering sample clust_sample_df = pd.DataFrame({\u0026#39;cluster\u0026#39;: clusters.reshape(-1), \u0026#39;cl_sample\u0026#39;:range(len(clusters))}) # creating an index with the sample used for clustering index = NNDescent(embed_sample, n_neighbors=10) # querying for all the data nn = index.query(sup_embed_et, k=1) # creating a dataframe with nearest neighbors for all samples to_cluster_df = pd.DataFrame({\u0026#39;sample\u0026#39;:range(sup_embed_et.shape[0]), \u0026#39;cl_sample\u0026#39;: nn[0].reshape(-1)}) # merging to assign cluster to all other samples, and tidying it final_cluster_df = to_cluster_df.merge(clust_sample_df, on=\u0026#39;cl_sample\u0026#39;) final_cluster_df = final_cluster_df.set_index(\u0026#39;sample\u0026#39;).sort_index() # plotting the embedding plt.figure(figsize=(12,7), dpi=150) plt.scatter(sup_embed_et[:,0], sup_embed_et[:,1], s=1, c=final_cluster_df[\u0026#39;cluster\u0026#39;], cmap=\u0026#39;plasma\u0026#39;) ## \u0026lt;matplotlib.collections.PathCollection object at 0x7f1b40c6ee48\u0026gt; plt.title(\u0026#39;Hierarchical clustering and extraTrees\u0026#39;) ## Text(0.5, 1.0, \u0026#39;Hierarchical clustering and extraTrees\u0026#39;) plt.xlabel(\u0026#39;$x_0$\u0026#39;); plt.ylabel(\u0026#39;$x_1$\u0026#39;) ## Text(0.5, 0, \u0026#39;$x_0$\u0026#39;) ## Text(0, 0.5, \u0026#39;$x_1$\u0026#39;) plt.show()  R Implementation(s) Now, in R create a data.frame using the same data we simulated with Python and split into train/test with sklearn‚Äôs train_test_split:\nlibrary(dplyr) ## ## Attaching package: \u0026#39;dplyr\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union library(reticulate) df \u0026lt;- data.frame(py$X) df$labels \u0026lt;- as.factor(py$y) # convert to factor for classification d_train \u0026lt;- data.frame(py$X_train) d_test \u0026lt;- data.frame(py$X_test) d_train$labels \u0026lt;- py$y_train d_test$labels \u0026lt;- py$y_test # Identity the response column ycol \u0026lt;- \u0026quot;labels\u0026quot; # Identify the predictor columns xcols \u0026lt;- setdiff(names(d_train), ycol) # Convert response to factor (required by randomForest) d_train[,ycol] \u0026lt;- as.factor(d_train[,ycol]) d_test[,ycol] \u0026lt;- as.factor(d_test[,ycol]) randomForest First, the {randomForest} package (one of the oldest, most well-known, but not most-optimal, packages):\nlibrary(randomForest) ## randomForest 4.6-14 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attaching package: \u0026#39;randomForest\u0026#39; ## The following object is masked from \u0026#39;package:dplyr\u0026#39;: ## ## combine library(cvAUC) ## Loading required package: ROCR ## Loading required package: data.table ## ## Attaching package: \u0026#39;data.table\u0026#39; ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## between, first, last ##  ## cvAUC version: 1.1.0 ## Notice to cvAUC users: Major speed improvements in version 1.1.0 ##  # Train a default RF model with 100 trees ## set set.seed(123) # For reproducibility system.time( model \u0026lt;- randomForest( x = d_train[,xcols], y = d_train[,ycol], xtest = d_test[,xcols], ntree = 100, nodes = TRUE # set to keep information on which trees in forest assigned to ) ) ## user: 30.835 system: 0.008 elapsed: 30.837 ## user system elapsed ## 40.846 0.041 40.932 # Generate predictions on test dataset preds \u0026lt;- model$test$votes[, 2] labels \u0026lt;- d_test[,ycol] # Compute AUC on the test set cvAUC::AUC(predictions = preds, labels = labels) ## [1] 0.8918756 The model performance is 0.8919 AUC. This is slightly better than what we saw in Python (albeit a bit slower)\nNow, as we did in Python, we need to train and apply it on the whole dataset, keeping track of which leaves in the forest each sample was assigned to.\nset.seed(123) # For reproducibility md_full \u0026lt;- randomForest(formula = labels ~ ., data = df, ntree = 100, keep.forest = TRUE) phat_full \u0026lt;- predict(md_full, newdata = df, type = \u0026quot;prob\u0026quot;, nodes = TRUE) # get the leaf indices that each sample was assigned to in the forest leaves \u0026lt;- attr(phat_full, \u0026quot;nodes\u0026quot;) dim(leaves) ## [1] 10000 100 For consistency, let‚Äôs take the data back into Python and plot it using matplotlib:\n# Assign R object as Python object leafs = r.leaves # Get embeddings from UMAP sup_embed_rf = UMAP(metric=\u0026#39;hamming\u0026#39;).fit_transform(leafs) # plotting the embedding ## /home/mtg/miniconda3/envs/r-reticulate/lib/python3.6/site-packages/umap/umap_.py:1762: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable ## \u0026quot;inverse_transform will be unavailable\u0026quot;.format(self.metric) plt.figure(figsize=(12,7), dpi=150) plt.scatter(sup_embed_rf[y == 0,0], sup_embed_rf[y == 0,1], s=1, c=\u0026#39;C0\u0026#39;, cmap=\u0026#39;viridis\u0026#39;, label=\u0026#39;$y=0$\u0026#39;) ## \u0026lt;matplotlib.collections.PathCollection object at 0x7f1af0bdd278\u0026gt; plt.scatter(sup_embed_rf[y == 1,0], sup_embed_rf[y == 1,1], s=1, c=\u0026#39;C1\u0026#39;, cmap=\u0026#39;viridis\u0026#39;, label=\u0026#39;$y=1$\u0026#39;) ## \u0026lt;matplotlib.collections.PathCollection object at 0x7f1b4091ce10\u0026gt; plt.title(\u0026#39;Supervised embedding with randomForest\u0026#39;) ## Text(0.5, 1.0, \u0026#39;Supervised embedding with randomForest\u0026#39;) plt.xlabel(\u0026#39;$x_0$\u0026#39;); plt.ylabel(\u0026#39;$x_1$\u0026#39;) ## Text(0.5, 0, \u0026#39;$x_0$\u0026#39;) ## Text(0, 0.5, \u0026#39;$x_1$\u0026#39;) plt.legend(fontsize=16, markerscale=5); plt.show() # taking a sample of the dataframe embed_sample = pd.DataFrame(sup_embed_rf).sample(5000, random_state=42) # running fastcluster hierarchical clustering on the improved embedding H = single(embed_sample) # getting the clusters clusters = cut_tree(H, height=0.35) print(\u0026#39;Number of clusters:\u0026#39;, len(np.unique(clusters))) ## Number of clusters: 14 This shows 12 clusters (although the simulated number was 20).\n# creating a dataframe for the clustering sample clust_sample_df = pd.DataFrame({\u0026#39;cluster\u0026#39;: clusters.reshape(-1), \u0026#39;cl_sample\u0026#39;:range(len(clusters))}) # creating an index with the sample used for clustering index = NNDescent(embed_sample, n_neighbors=10) # querying for all the data nn = index.query(sup_embed_et, k=1) # creating a dataframe with nearest neighbors for all samples to_cluster_df = pd.DataFrame({\u0026#39;sample\u0026#39;:range(sup_embed_et.shape[0]), \u0026#39;cl_sample\u0026#39;: nn[0].reshape(-1)}) # merging to assign cluster to all other samples, and tidying it final_cluster_df = to_cluster_df.merge(clust_sample_df, on=\u0026#39;cl_sample\u0026#39;) final_cluster_df = final_cluster_df.set_index(\u0026#39;sample\u0026#39;).sort_index() # plotting the embedding plt.figure(figsize=(12,7), dpi=150) plt.scatter(sup_embed_et[:,0], sup_embed_et[:,1], s=1, c=final_cluster_df[\u0026#39;cluster\u0026#39;], cmap=\u0026#39;plasma\u0026#39;) ## \u0026lt;matplotlib.collections.PathCollection object at 0x7f1b40c0d358\u0026gt; plt.title(\u0026#39;Hierarchical clustering and randomForest\u0026#39;) ## Text(0.5, 1.0, \u0026#39;Hierarchical clustering and randomForest\u0026#39;) plt.xlabel(\u0026#39;$x_0$\u0026#39;); plt.ylabel(\u0026#39;$x_1$\u0026#39;) ## Text(0.5, 0, \u0026#39;$x_0$\u0026#39;) ## Text(0, 0.5, \u0026#39;$x_1$\u0026#39;) plt.show() Other R packages are known to outperform {randomForest} - both in terms of accuracy and speed - so I would also like to get this information from {ranger} and {ExtraTrees}. ranger for example, has excellent speed and support for high-dimensional or wide data (e.g. scRNA-sequencing data)\n ranger Here‚Äôs what I‚Äôve got so far with the ranger method:\nlibrary(ranger) ## ## Attaching package: \u0026#39;ranger\u0026#39; ## The following object is masked from \u0026#39;package:randomForest\u0026#39;: ## ## importance library(pROC) ## Type \u0026#39;citation(\u0026quot;pROC\u0026quot;)\u0026#39; for a citation. ## ## Attaching package: \u0026#39;pROC\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## cov, smooth, var set.seed(123) # ranger speed system.time( df_ranger \u0026lt;- ranger( formula = labels ~ ., data = d_train, num.trees = 100, num.threads = 1, # default is the number of CPUs on machine probability = TRUE ) ) # user 13.047 system: 0.01 elapsed: 13.048 ## user system elapsed ## 17.647 0.006 17.672 pred.ranger \u0026lt;- predict(df_ranger, data = d_test, type = \u0026quot;terminalNodes\u0026quot;) # get model accuracy ranger.roc \u0026lt;- roc(d_test$labels, pred.ranger$predictions[,2]) ## Setting levels: control = 0, case = 1 ## Setting direction: controls \u0026gt; cases pROC::auc(ranger.roc) ## Area under the curve: 0.5741 This model didn‚Äôt perform SO well with 0.5471 AUC. DO YOU THINK THIS COULD BE DUE TO pROC WHAT IT IS RECORD??\nset.seed(123) df_ranger \u0026lt;- ranger(formula = labels ~ ., data = df, num.trees = 100, probability = TRUE) pred.ranger \u0026lt;- predict(df_ranger, data = df, type = \u0026quot;terminalNodes\u0026quot;) lyves \u0026lt;- pred.ranger$predictions Now in enter repl_python() and in Python enter:\n# Assign R object as Python object lyves = r.lyves # Get embeddings from UMAP sup_embed_rg = UMAP(metric=\u0026#39;hamming\u0026#39;).fit_transform(lyves) # plotting the embedding ## /home/mtg/miniconda3/envs/r-reticulate/lib/python3.6/site-packages/umap/umap_.py:1762: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable ## \u0026quot;inverse_transform will be unavailable\u0026quot;.format(self.metric) plt.figure(figsize=(12,7), dpi=150) plt.scatter(sup_embed_rg[y == 0,0], sup_embed_rg[y == 0,1], s=1, c=\u0026#39;C0\u0026#39;, cmap=\u0026#39;viridis\u0026#39;, label=\u0026#39;$y=0$\u0026#39;) ## \u0026lt;matplotlib.collections.PathCollection object at 0x7f1b12d8b5f8\u0026gt; plt.scatter(sup_embed_rg[y == 1,0], sup_embed_rg[y == 1,1], s=1, c=\u0026#39;C1\u0026#39;, cmap=\u0026#39;viridis\u0026#39;, label=\u0026#39;$y=1$\u0026#39;) ## \u0026lt;matplotlib.collections.PathCollection object at 0x7f1af87baba8\u0026gt; plt.title(\u0026#39;Supervised embedding with ranger\u0026#39;) ## Text(0.5, 1.0, \u0026#39;Supervised embedding with ranger\u0026#39;) plt.xlabel(\u0026#39;$x_0$\u0026#39;); plt.ylabel(\u0026#39;$x_1$\u0026#39;) ## Text(0.5, 0, \u0026#39;$x_0$\u0026#39;) ## Text(0, 0.5, \u0026#39;$x_1$\u0026#39;) plt.legend(fontsize=16, markerscale=5); plt.show() # taking a sample of the dataframe embed_sample = pd.DataFrame(sup_embed_rg).sample(5000, random_state=42) # running fastcluster hierarchical clustering on the improved embedding H = single(embed_sample) # getting the clusters clusters = cut_tree(H, height=0.35) print(\u0026#39;Number of clusters:\u0026#39;, len(np.unique(clusters))) ## Number of clusters: 17 This shows 14 clusters (although the simulated number was 20).\n# creating a dataframe for the clustering sample clust_sample_df = pd.DataFrame({\u0026#39;cluster\u0026#39;: clusters.reshape(-1), \u0026#39;cl_sample\u0026#39;:range(len(clusters))}) # creating an index with the sample used for clustering index = NNDescent(embed_sample, n_neighbors=10) # querying for all the data nn = index.query(sup_embed_et, k=1) # creating a dataframe with nearest neighbors for all samples to_cluster_df = pd.DataFrame({\u0026#39;sample\u0026#39;:range(sup_embed_et.shape[0]), \u0026#39;cl_sample\u0026#39;: nn[0].reshape(-1)}) # merging to assign cluster to all other samples, and tidying it final_cluster_df = to_cluster_df.merge(clust_sample_df, on=\u0026#39;cl_sample\u0026#39;) final_cluster_df = final_cluster_df.set_index(\u0026#39;sample\u0026#39;).sort_index() # plotting the embedding plt.figure(figsize=(12,7), dpi=150) plt.scatter(sup_embed_et[:,0], sup_embed_et[:,1], s=1, c=final_cluster_df[\u0026#39;cluster\u0026#39;], cmap=\u0026#39;plasma\u0026#39;) ## \u0026lt;matplotlib.collections.PathCollection object at 0x7f1aee1ad588\u0026gt; plt.title(\u0026#39;Hierarchical clustering and ranger\u0026#39;) ## Text(0.5, 1.0, \u0026#39;Hierarchical clustering and ranger\u0026#39;) plt.xlabel(\u0026#39;$x_0$\u0026#39;); plt.ylabel(\u0026#39;$x_1$\u0026#39;) ## Text(0.5, 0, \u0026#39;$x_0$\u0026#39;) ## Text(0, 0.5, \u0026#39;$x_1$\u0026#39;) plt.show()  extraTrees The {extraTrees} package, may achieve the closest comparison to scikit-learn‚Äôs ExtraTreesClassifier function.\nlibrary(extraTrees) ## Loading required package: rJava y \u0026lt;- \u0026quot;labels\u0026quot; characteristics \u0026lt;- setdiff(names(d_train), y) train \u0026lt;- d_train[, characteristics] test \u0026lt;- d_test[, characteristics] set.seed(123) system.time({ model_extraTrees \u0026lt;- extraTrees(x = train, y = as.factor(d_train$labels), ntree = 500, numThreads = 1 # must be set explicitly as the default is 1 ) }) # user 10.184 system: 0.104 elapsed: 9.770 ## user system elapsed ## 14.276 0.146 13.468 However, unlike it‚Äôs Python counterpart, at this time one cannot get the lead indices from {extraTrees} so I made a feature request.\nIf you find this article useful feel free to share it with others or recommend this article! üòÑ\nAs always, if you have any questions or comments feel free to leave your feedback below or you can always reach me on LinkedIn. Till then, see you in the next post! üôá‚Äç‚ôÄ\n   \n // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); });   $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown  .nav-tabs  li').click(function () { $(this).parent().toggleClass('nav-tabs-open') }); });   (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();   \n","date":1525672800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525672800,"objectID":"8ea76530e4d2b23df61593c667b88517","permalink":"/project/free-ride-world-tour/","publishdate":"2018-05-07T00:00:00-06:00","relpermalink":"/project/free-ride-world-tour/","section":"project","summary":"Using the TwitteR package and web-scraping the FWT website for better insights","tags":["R","Web scraping","dataviz","API"],"title":"Analyzing 12 years of the Free Ride World Tour","type":"project"},{"authors":null,"categories":null,"content":"","date":1520665200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520665200,"objectID":"44d9666beba54e6a51f6b1436dc8c31a","permalink":"/project/gsea/","publishdate":"2018-03-10T00:00:00-07:00","relpermalink":"/project/gsea/","section":"project","summary":"Techniques for the analysis of gene set enrichments, pathway analysis, gene ontologies, functional analysis of metabolomic profiling and coexpression networks","tags":["R","dataviz","ggplot2"],"title":"Gene set enrichment in R","type":"project"},{"authors":[],"categories":null,"content":"Click on the Slides button above to view the built-in slides feature.\n Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483254000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483254000,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00-07:00","relpermalink":"/talk/example/","section":"talk","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam.","tags":[],"title":"Example Talk","type":"talk"}]